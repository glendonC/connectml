[
  {
    "id": "standard_scaler",
    "name": "Standard Scaler",
    "type": "preprocessing",
    "description": "Normalizes numeric features to zero mean and unit variance.",
    "code_snippet": "from sklearn.preprocessing import StandardScaler",
    "model_asset": "/models/standard_scaler.glb",
    "icon": "ðŸ“Š",
    "suggested_alternatives": ["min_max_scaler", "robust_scaler"],
    "requirements": {
      "dependencies": ["scikit-learn>=1.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "DataCleanerGPT",
      "role": "Data Preprocessing Expert",
      "quote": "Clean data is my jam! Let me handle those messy sensor readings.",
      "why_chosen": "Essential for neural networks and algorithms sensitive to feature scales. Standardization ensures all features contribute equally to the model's learning process."
    }
  },
  {
    "id": "min_max_scaler",
    "name": "Min-Max Scaler",
    "type": "preprocessing",
    "description": "Scales values to a 0-1 range. Useful for bounded inputs.",
    "code_snippet": "from sklearn.preprocessing import MinMaxScaler",
    "model_asset": "/models/min_max.glb",
    "icon": "ðŸ“ˆ",
    "requirements": {
      "dependencies": ["scikit-learn>=1.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "NormalizerGPT",
      "role": "Data Scaling Specialist",
      "quote": "Let me squeeze those values into a nice, bounded range!",
      "why_chosen": "Perfect for data with known bounds or when preserving zero values is important. Particularly useful for image processing and neural network output layers."
    }
  },
  {
    "id": "outlier_filter",
    "name": "Outlier Detection",
    "type": "preprocessing",
    "description": "Removes anomalous data points using IQR or z-score methods.",
    "code_snippet": "df = df[(z_scores < 3).all(axis=1)]",
    "model_asset": "/models/outlier_filter.glb",
    "icon": "ðŸš¨",
    "requirements": {
      "dependencies": ["numpy>=1.20.0", "pandas>=1.3.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "AnomalyHunterGPT",
      "role": "Outlier Detection Expert",
      "quote": "I've got a keen eye for data that doesn't belong!",
      "why_chosen": "Critical for maintaining model robustness by identifying and removing statistical anomalies. Helps prevent model bias from extreme values and improves overall prediction reliability."
    }
  },
  {
    "id": "time_alignment",
    "name": "Time Alignment",
    "type": "preprocessing",
    "description": "Aligns multi-sensor time series to a shared timestamp index.",
    "code_snippet": "df = df.resample('1H').mean()",
    "model_asset": "/models/time_align.glb",
    "icon": "â±ï¸",
    "requirements": {
      "dependencies": ["pandas>=1.3.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "TimeKeeperGPT",
      "role": "Temporal Alignment Specialist",
      "quote": "Time waits for no one, but I'll make sure your data stays in sync!",
      "why_chosen": "Crucial for multi-sensor systems where data streams need precise temporal alignment. Ensures accurate correlation analysis and prevents temporal bias in model training."
    }
  },
  {
    "id": "transformer_model",
    "name": "Time Series Transformer",
    "type": "model",
    "description": "Captures long-term temporal patterns using attention mechanisms.",
    "code_snippet": "output = self.transformer(x)",
    "model_asset": "/models/transformer.glb",
    "icon": "ðŸ”",
    "requirements": {
      "dependencies": ["torch>=1.10.0", "transformers>=4.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "ModelArchitectGPT",
      "role": "Neural Architecture Specialist",
      "quote": "Time series data? Leave it to my transformer architecture!",
      "why_chosen": "Selected for its superior ability to capture long-range dependencies in sequential data. The self-attention mechanism excels at modeling complex temporal relationships."
    }
  },
  {
    "id": "json_exporter",
    "name": "JSON Output Formatter",
    "type": "postprocessing",
    "description": "Formats predictions as JSON with timestamps and metadata.",
    "code_snippet": "json.dumps(predictions)",
    "model_asset": "/models/json_formatter.glb",
    "icon": "ðŸ“¤",
    "requirements": {
      "dependencies": ["python-json-logger>=2.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "OutputOptimizerGPT",
      "role": "Prediction Refinement Specialist",
      "quote": "Let me format those predictions for easy integration!",
      "why_chosen": "Enables seamless integration with downstream services through standardized JSON output. Adds crucial metadata and timestamps for proper prediction tracking."
    }
  },
  {
    "id": "pca",
    "name": "PCA",
    "type": "feature",
    "description": "Applies Principal Component Analysis for dimensionality reduction.",
    "code_snippet": "from sklearn.decomposition import PCA; pca = PCA(n_components=10); X = pca.fit_transform(X)",
    "model_asset": "/models/pca.glb",
    "icon": "ðŸ“‰",
    "requirements": {
      "dependencies": ["scikit-learn>=1.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "FeatureEngineerGPT",
      "role": "Feature Engineering Specialist",
      "quote": "Let's distill those features down to their essence!",
      "why_chosen": "Optimal for reducing high-dimensional data while preserving maximum variance. Helps combat the curse of dimensionality and speeds up downstream model training."
    }
  },
  {
    "id": "data_validator",
    "name": "Data Validator",
    "type": "transformation",
    "description": "Validates input data against a predefined schema.",
    "code_snippet": "jsonschema.validate(data, schema)",
    "model_asset": "/models/data_validator.glb",
    "icon": "âœ…",
    "requirements": {
      "dependencies": ["jsonschema>=4.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "QualityControlGPT",
      "role": "Data Quality Guardian",
      "quote": "Ensuring your data is pristine and ready for analysis!",
      "why_chosen": "Prevents data quality issues by enforcing strict schema validation. Catches malformed inputs early in the pipeline to ensure downstream reliability."
    }
  },
  {
    "id": "model_drift_detector",
    "name": "Model Drift Detector",
    "type": "monitoring",
    "description": "Detects drift in model performance over time.",
    "code_snippet": "drift_detector.detect(predictions, actuals)",
    "model_asset": "/models/drift_detector.glb",
    "icon": "âš™ï¸",
    "requirements": {
      "dependencies": ["tensorflow>=2.0.0", "alibi_detect>=0.8.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "PerformanceMonitorGPT",
      "role": "Model Performance Analyst",
      "quote": "Keeping a vigilant eye on your model's health and stability.",
      "why_chosen": "Essential for production ML systems to detect performance degradation early. Monitors statistical drift in both feature distributions and model outputs."
    }
  },
  {
    "id": "shap_explainer",
    "name": "SHAP Explainer",
    "type": "explainability",
    "description": "Explains model predictions using SHAP values.",
    "code_snippet": "shap.summary_plot(shap_values, X)",
    "model_asset": "/models/shap_explainer.glb",
    "icon": "ðŸ’¡",
    "requirements": {
      "dependencies": ["shap>=0.39.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "InsightfulAnalystGPT",
      "role": "Model Interpretation Expert",
      "quote": "Unlocking the secrets behind your model's decisions.",
      "why_chosen": "Provides transparent, theoretically-grounded explanations for model predictions. Essential for building trust and meeting regulatory requirements."
    }
  },
  {
    "id": "robust_scaler",
    "name": "Robust Scaler",
    "type": "preprocessing",
    "description": "Scales data using statistics that are robust to outliers.",
    "code_snippet": "from sklearn.preprocessing import RobustScaler; scaler = RobustScaler(); X = scaler.fit_transform(X)",
    "model_asset": "/models/robust_scaler.glb",
    "icon": "ðŸ’ª",
    "requirements": {
      "dependencies": ["scikit-learn>=1.0.0"],
      "environments": ["Python 3.7+"]
    },
    "agent": {
      "name": "DataResilienceGPT",
      "role": "Outlier-Resistant Scaling Specialist",
      "quote": "Making your data robust against the slings and arrows of outliers!",
      "why_chosen": "Ideal for datasets with outliers that can't be removed. Uses quartile statistics instead of mean/variance for more robust feature scaling."
    }
  }
] 